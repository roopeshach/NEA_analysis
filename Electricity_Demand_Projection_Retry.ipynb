{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "bcf0ad39-2c3d-511c-b38e-89aa4d9aa29e",
        "openai_ephemeral_user_id": "1123a54d-765a-51bd-ae55-8727b6873964",
        "openai_subdivision1_iso_code": "NP-P4"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "bd792c28-36e6-41c4-8e94-621ad4f14db0",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5836cc2b-8efa-4ff7-8998-c083574a266d"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:55:50.058421+00:00",
          "start_time": "2023-09-12T19:55:47.508199+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install -q tensorflow pandas numpy scikit-learn",
      "outputs": []
    },
    {
      "id": "5cb4cdb9-0c20-47bb-a866-0f73d5a55f38",
      "cell_type": "markdown",
      "source": "### 1. Data Preprocessing\n\n#### 1.1 Load the Dataset\nIn this step, we'll load the electricity demand dataset from the provided Excel file.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "626048a4-7b37-4c5b-83f6-695a8169b09b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "78cc6fd1-8912-42af-aa96-9b3266b91a72"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:56:59.835709+00:00",
          "start_time": "2023-09-12T19:56:59.359729+00:00"
        },
        "datalink": {
          "7645a3b8-831d-4769-bcc6-768dacd43202": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 11,
              "orig_num_rows": 5,
              "orig_size_bytes": 480,
              "truncated_num_cols": 11,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 480,
              "truncated_string_columns": []
            },
            "display_id": "7645a3b8-831d-4769-bcc6-768dacd43202",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-12T19:56:59.668884",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_f878953d7c574ec096cbb20e8cef4896"
          }
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\n\n# Load the dataset from the Excel file\ndata_path = 'data.xlsx'\ndata = pd.read_excel(data_path)\n\n# Display the first few rows of the dataset\ndata.head()",
      "outputs": []
    },
    {
      "id": "b165857d-e34d-4e3f-9081-f65857a1d7b5",
      "cell_type": "markdown",
      "source": "#### 1.2 Data Cleaning\nIn this step, we'll clean the dataset by performing the following tasks:\n- Remove unnecessary columns.\n- Handle missing values.\n- Convert data types if necessary.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "0be0c8be-107d-4f96-9add-1b8c46714405",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1b4bf9ca-ee14-4f31-9d45-e62befd3f699"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:57:43.657325+00:00",
          "start_time": "2023-09-12T19:57:43.457510+00:00"
        },
        "datalink": {
          "9d3b98bc-7232-4132-a59e-2c1301bf4c76": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 6,
              "orig_num_rows": 5,
              "orig_size_bytes": 280,
              "truncated_num_cols": 6,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 280,
              "truncated_string_columns": []
            },
            "display_id": "9d3b98bc-7232-4132-a59e-2c1301bf4c76",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-12T19:57:43.495916",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_4ed66d20fb7145b795781ec147e051d6"
          }
        }
      },
      "execution_count": null,
      "source": "# Drop unnecessary columns\ndata_cleaned = data.drop(columns=['S. No.', 'Date (BS)', 'Unnamed: 5', 'Unnamed: 6', 'Remarks'])\n\n# Drop rows with missing values\ndata_cleaned = data_cleaned.dropna()\n\n# Convert 'Temperature (celcius)' column to numeric\ndata_cleaned['Temperature (celcius)'] = pd.to_numeric(data_cleaned['Temperature (celcius)'], errors='coerce')\n\n# Display the cleaned dataset\ndata_cleaned.head()",
      "outputs": []
    },
    {
      "id": "9d42c9c9-17ae-47ea-aa85-8dc28a885380",
      "cell_type": "markdown",
      "source": "#### 1.3 Normalize the Data\nTo ensure that all features are on a similar scale, we'll normalize the data. This step is crucial for deep learning models as it helps in faster convergence and better performance. We'll use the `MinMaxScaler` from scikit-learn to scale the features between 0 and 1.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "0afa1402-9024-4fe7-93c3-3fd47db613d6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4f86cd8f-ce41-482b-bb44-6f54c4ca7bf3"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:58:21.923622+00:00",
          "start_time": "2023-09-12T19:58:21.468036+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.preprocessing import MinMaxScaler\n\n# Define the features and target variable\nfeatures = data_cleaned.drop(columns=['Date (CE)', 'Peak Demand  (megawatt)'])\ntarget = data_cleaned['Peak Demand  (megawatt)']\n\n# Initialize the MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the features\nscaled_features = scaler.fit_transform(features)\n\n# Display the first few rows of scaled features\nscaled_features[:5]",
      "outputs": []
    },
    {
      "id": "07cfa617-c39d-4136-a4de-ea42a55ea279",
      "cell_type": "markdown",
      "source": "#### 1.4 Split the Dataset\nIn this step, we'll divide the dataset into training, validation, and test sets. This division allows us to train our models on one subset of the data and validate and test them on separate, unseen subsets. This approach ensures a more robust evaluation of the model's performance.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "a39b2b54-9033-44c8-b328-194848b8cd7f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d61dc541-a64e-4225-8db6-546e505a756e"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:59:01.576332+00:00",
          "start_time": "2023-09-12T19:59:01.401423+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\n\n# Split the data into training and temporary sets (80% training, 20% temporary)\nX_temp, X_test, y_temp, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42, shuffle=False)\n\n# Split the temporary set into validation and test sets (50% validation, 50% test)\nX_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n\n# Display the shape of the training, validation, and test sets\nX_train.shape, X_valid.shape, X_test.shape",
      "outputs": []
    },
    {
      "id": "2ce6e946-e458-4443-8f43-f4b177c11070",
      "cell_type": "markdown",
      "source": "### 2. Model Building\n\n#### 2.1 LSTM Model\nIn this step, we'll define the architecture of the LSTM model. LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) that is well-suited for time series forecasting. We'll specify the input shape based on our data and compile the model with an optimizer and loss function.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "1edbb296-84b4-4098-8c4d-fc299beb13b6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6428a8ae-6777-4b5b-877a-24a99d664cc6"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T19:59:58.445617+00:00",
          "start_time": "2023-09-12T19:59:58.072972+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the LSTM model architecture\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the LSTM model\nlstm_model.compile(optimizer='adam', loss='mse')\n\n# Display the LSTM model summary\nlstm_model.summary()",
      "outputs": []
    },
    {
      "id": "9a63803e-1414-492c-bc20-b43a3b0eac66",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c2735454-0ae8-42f5-b056-dd1f57cd2715"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:00:22.313012+00:00",
          "start_time": "2023-09-12T20:00:20.524554+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow as tf\n\n# Define the LSTM model architecture\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the LSTM model\nlstm_model.compile(optimizer='adam', loss='mse')\n\n# Display the LSTM model summary\nlstm_model.summary()",
      "outputs": []
    },
    {
      "id": "687783c2-4c51-4775-b64c-23f2f3d80597",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e07deb76-5fdc-47ef-8d85-40c146de724a"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:00:58.537192+00:00",
          "start_time": "2023-09-12T20:00:58.008954+00:00"
        }
      },
      "execution_count": null,
      "source": "# Reshape the data to have a single timestep per sample\nX_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_valid_reshaped = X_valid.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\nX_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n\n# Define the LSTM model architecture with the reshaped input\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the LSTM model\nlstm_model.compile(optimizer='adam', loss='mse')\n\n# Display the LSTM model summary\nlstm_model.summary()",
      "outputs": []
    },
    {
      "id": "fe6ea32d-bee5-4859-8b2e-e43c85b17bd5",
      "cell_type": "markdown",
      "source": "#### 2.2 Train the LSTM Model\nIn this step, we'll train the LSTM model using the training dataset. We'll also validate the model using the validation dataset. To prevent overfitting and ensure efficient training, we'll implement early stopping, which will monitor the validation loss and stop training once it starts increasing.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b337912e-101f-4417-9975-950ffeecfa54",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "93f304d8-5a5d-4af5-86fe-45ff25bd4eda"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:02:04.014834+00:00",
          "start_time": "2023-09-12T20:01:50.708597+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define early stopping to monitor validation loss\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the LSTM model\nlstm_history = lstm_model.fit(\n    X_train_reshaped, y_train,\n    epochs=100,\n    validation_data=(X_valid_reshaped, y_valid),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Display the training and validation loss for each epoch\nlstm_history.history",
      "outputs": []
    },
    {
      "id": "0363d995-035d-4e10-8f6e-dfe2ecef3ed5",
      "cell_type": "markdown",
      "source": "#### 2.3 GRU Model\nIn this step, we'll define the architecture of the GRU model. GRU (Gated Recurrent Unit) is a type of recurrent neural network (RNN) that's often considered a simplified version of the LSTM. It's designed to solve the vanishing gradient problem of traditional RNNs. We'll specify the input shape based on our data and compile the model with an optimizer and loss function.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "bc82f36c-3a3d-4123-94b4-13e952959bb5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3f962248-9cc4-449e-a145-b026491bd934"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:03:48.430278+00:00",
          "start_time": "2023-09-12T20:03:48.188075+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the GRU model architecture\ngru_model = tf.keras.Sequential([\n    tf.keras.layers.GRU(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the GRU model\ngru_model.compile(optimizer='adam', loss='mse')\n\n# Display the GRU model summary\ngru_model.summary()",
      "outputs": []
    },
    {
      "id": "2d9647a9-6b1f-49c6-a787-135d18dd5213",
      "cell_type": "markdown",
      "source": "#### 2.4 Train the GRU Model\nIn this step, we'll train the GRU model using the training dataset. We'll also validate the model using the validation dataset. As with the LSTM model, we'll implement early stopping to monitor the validation loss and prevent overfitting.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b8c70531-f9bd-4c86-88ba-e4ee980a5223",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "866d6e73-b960-455c-b987-4a9a99047b17"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:05:14.813975+00:00",
          "start_time": "2023-09-12T20:05:00.871430+00:00"
        }
      },
      "execution_count": null,
      "source": "# Train the GRU model\ngru_history = gru_model.fit(\n    X_train_reshaped, y_train,\n    epochs=100,\n    validation_data=(X_valid_reshaped, y_valid),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Display the training and validation loss for each epoch\ngru_history.history",
      "outputs": []
    },
    {
      "id": "66f3b0f6-6ecf-4e61-a9fb-ea8cb337e877",
      "cell_type": "markdown",
      "source": "### 3. Evaluation\n\n#### 3.1 Evaluate the LSTM Model\nIn this step, we'll evaluate the LSTM model's performance on the test set. We'll calculate key metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). Also, we'll visualize the actual vs. predicted electricity demand to visually assess the model's accuracy.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "72ce0032-4cd2-4b60-8641-688c4f946af4",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1355393a-6e9e-459e-bede-7d4a5a90aa04"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:06:40.028178+00:00",
          "start_time": "2023-09-12T20:06:39.689605+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# Predict using the LSTM model\nlstm_predictions = lstm_model.predict(X_test_reshaped)\n\n# Calculate evaluation metrics for the LSTM model\nlstm_mae = mean_absolute_error(y_test, lstm_predictions)\nlstm_mse = mean_squared_error(y_test, lstm_predictions)\nlstm_rmse = np.sqrt(lstm_mse)\n\nlstm_mae, lstm_mse, lstm_rmse",
      "outputs": []
    },
    {
      "id": "a424ee9d-8835-4f25-9840-7788083337ee",
      "cell_type": "markdown",
      "source": "#### 3.2 Evaluate the GRU Model\nNext, we'll evaluate the GRU model's performance on the test set. Similar to the LSTM model, we'll calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for the GRU model. This will allow us to compare the performance of both models.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "8813c21d-7561-4abc-bb38-1656fac9daef",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "060e9b28-2edb-45d7-b35c-06e7fd74f585"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:07:34.075967+00:00",
          "start_time": "2023-09-12T20:07:33.756436+00:00"
        }
      },
      "execution_count": null,
      "source": "# Predict using the GRU model\ngru_predictions = gru_model.predict(X_test_reshaped)\n\n# Calculate evaluation metrics for the GRU model\ngru_mae = mean_absolute_error(y_test, gru_predictions)\ngru_mse = mean_squared_error(y_test, gru_predictions)\ngru_rmse = np.sqrt(gru_mse)\n\ngru_mae, gru_mse, gru_rmse",
      "outputs": []
    },
    {
      "id": "6f1c9d01-e6ad-4354-9509-bcd75ca2b907",
      "cell_type": "markdown",
      "source": "#### 3.3 Visualize Actual vs. Predicted Values\nTo visually assess the performance of the LSTM and GRU models, we'll plot the actual electricity demand against the predicted values. This visualization will provide a clear picture of how closely the predictions align with the actual values.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5de7b0b6-2962-43a4-a8a8-1ee4c7766d32",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3668a4c8-c348-4c27-848e-3b157ab473c4"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:08:44.201267+00:00",
          "start_time": "2023-09-12T20:08:43.264238+00:00"
        }
      },
      "execution_count": null,
      "source": "import matplotlib.pyplot as plt\n\n# Plot actual vs. predicted values for LSTM and GRU models\nplt.figure(figsize=(15, 6))\n\n# Plotting for LSTM model\nplt.subplot(1, 2, 1)\nplt.plot(y_test, label='Actual', color='blue')\nplt.plot(lstm_predictions, label='Predicted', color='red', alpha=0.7)\nplt.title('LSTM Model: Actual vs. Predicted')\nplt.legend()\n\n# Plotting for GRU model\nplt.subplot(1, 2, 2)\nplt.plot(y_test, label='Actual', color='blue')\nplt.plot(gru_predictions, label='Predicted', color='green', alpha=0.7)\nplt.title('GRU Model: Actual vs. Predicted')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
      "outputs": []
    },
    {
      "id": "7b0cc201-d19e-49e2-a2e5-16e60d98f708",
      "cell_type": "markdown",
      "source": "### 4. Results & Comparison\n\nIn this section, we'll compare the performance metrics of the LSTM and GRU models. We'll analyze aspects such as the error metrics, training time, and responsiveness to hyperparameters. This comparison will provide insights into which model is more suitable for the dataset and the specific problem of electricity demand projection.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "cf66a20d-2324-4421-9ce2-ba84e816a01b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bcdc696b-5b09-4d33-bab8-5caef6adfa0d"
        },
        "ExecuteTime": {
          "end_time": "2023-09-12T20:09:57.368825+00:00",
          "start_time": "2023-09-12T20:09:57.152263+00:00"
        },
        "datalink": {
          "44b16cd8-1135-4df9-a6f1-8ae267fac3b9": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 4,
              "orig_num_rows": 2,
              "orig_size_bytes": 80,
              "truncated_num_cols": 4,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 80,
              "truncated_string_columns": []
            },
            "display_id": "44b16cd8-1135-4df9-a6f1-8ae267fac3b9",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-12T20:09:57.208521",
            "user_variable_name": "results_df",
            "variable_name": "results_df"
          }
        }
      },
      "execution_count": null,
      "source": "# Compare the performance metrics of LSTM and GRU models\nresults_comparison = {\n    'Model': ['LSTM', 'GRU'],\n    'MAE': [lstm_mae, gru_mae],\n    'MSE': [lstm_mse, gru_mse],\n    'RMSE': [lstm_rmse, gru_rmse]\n}\n\nresults_df = pd.DataFrame(results_comparison)\nresults_df",
      "outputs": []
    },
    {
      "id": "ff95e309-9c8b-48b6-b889-fe803de84dca",
      "cell_type": "markdown",
      "source": "### 5. Conclusion\n\nBased on our analysis and evaluation, we can draw the following conclusions:\n\n- Both the LSTM and GRU models performed well in predicting electricity demand, capturing the patterns in the data effectively.\n- The LSTM model showed slightly better performance based on error metrics (MAE, MSE, RMSE) compared to the GRU model.\n- While both models can be used for this problem, the LSTM model might be a more suitable choice given its performance on this dataset.\n- Potential improvements or further experiments could include tuning hyperparameters, using a more complex architecture, or incorporating additional features into the dataset.\n\n",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}